{"model":"codellama:7b","created_at":"2024-06-24T05:08:24.0338137Z","message":{"role":"assistant","content":"\n# Llama Node.js Terminal Interface\n\nThis is a markdown readme file for the Git repo relating to this Ollama node.js terminal interface where you can provide flags for different models, including Llava-phi3:latest, Codellama:7b, and Llama3:latest.\n\n## Running the project\n\nTo run the project, follow these steps:\n\n1. Install Node.js and npm if you haven't already.\n2. Clone this repo to your local machine using `git clone`.\n3. Navigate to the cloned repo folder in your terminal or command prompt.\n4. Run `npm install` to install the project dependencies.\n5. Start the app by running `npm start`.\n6. To stop the app, press `Ctrl + C` in your terminal or command prompt.\n\n## Using the project\n\nTo use the project, follow these steps:\n\n1. Open a terminal or command prompt and navigate to the cloned repo folder.\n2. Run `npm start`.\n3. The app will be running on `http://localhost:3000` by default. You can also specify a different port using the `--port` flag, e.g., `npm start -- --port 8080`.\n4. To access the API endpoints, send HTTP requests to `http://localhost:<port>/api/<endpoint>` where `<port>` is the port number you specified in step 3 and `<endpoint>` is one of the following:\n\t* `model` - Get a list of available models.\n\t* `predict` - Make a prediction using a specific model.\n5. You can also specify a different model using the `--model` flag, e.g., `npm start -- --model llava-phi3:latest`.\n6. To stop the app, press `Ctrl + C` in your terminal or command prompt.\n\n### Available models\n\nThe following models are currently available for use with this project:\n\n* `llama-phi3:latest`\n* `codellama:7b`\n* `llama3:latest`\n\nYou can specify any of these models using the `--model` flag, e.g., `npm start -- --model llava-phi3:latest`.\n\n### Prediction endpoint\n\nTo make a prediction using a specific model, send an HTTP POST request to `http://localhost:<port>/api/predict` with a JSON payload containing the following fields:\n\n* `input`: The input data for the prediction. This should be a list of numbers or a list of lists of numbers.\n* `model`: The name of the model you want to use for the prediction. You can specify any of the available models using the `--model` flag, e.g., `npm start -- --model llava-phi3:latest`.\n\nFor example, to make a prediction using the `llama-phi3:latest` model, you can send an HTTP POST request with the following JSON payload:\n```json\n{\n  \"input\": [1, 2, 3],\n  \"model\": \"llama-phi3:latest\"\n}\n```\nThe app will respond with a JSON object containing the prediction results."},"done_reason":"stop","done":true,"total_duration":7203180200,"load_duration":10797500,"prompt_eval_count":62,"prompt_eval_duration":270708000,"eval_count":715,"eval_duration":6914059000}